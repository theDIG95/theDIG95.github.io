---
title: Putting the GPU to Work - PyCuda
layout: post
icon : fa-code 
---

With the increased availability and more affordable prices of general purpose GPUs it makes sense to leverage their power to accelerate our applications. Currently Nvidia is at the forefront of devices that provide high computational capabilities. From dedicated super-computing devices to general purpose GPUs. Also with the CUDA API it makes it easier to interface with such devices for applications other than graphics processing.  
The [PyCuda](https://mathema.tician.de/software/pycuda/) library exposes the CUDA library to the Python programming language. It exposes the full CUDA API and also provides abstractions for common parts which makes the development more continent. It also understands and is compatible with the Numpy library so the learning curve is not steep.

## Abstracton of kernels  

The PyCuda library contains the `gpuarray`objects which are similar to and compatible with the Numpy's `ndarray`. Using these arrays we can abstract away the transfer of objects to and from the device and host. These arrays also support operations with which we do not have to write kernels for CUDA.  
First we import all the necessary modules.

```python
import pycuda.driver as cuda_driver
import pycuda.autoinit
import pycuda.gpuarray as gpuarray
```

The `pycuda.autoinit` automatically sets up the device and makes it ready for use.  
Now we can use it to double the elements inside an array

```python
def gpu_arrs(arr):
    arr_gpu = gpuarray.to_gpu(arr)
    arr_doubled = (arr_gpu*2).get()
```

Now we can pass a numpy array of appropriate data type to it and it will double the array.

## Writing and Using Kernels  

We can also write custom kernels to perform operations we want.

```python
from pycuda.compiler import SourceModule

def custom_kernel(arr):
    arr_doubled = arr*2

    kernel = SourceModule(
        """__global__ void doubled(float *a)
        {
        int idx = threadIdx.x + threadIdx.y*3;
        a[idx] *= 2;
        }
        """)

    gpu_func = kernel.get_function("doubled")
    gpu_func(
        cuda_driver.InOut(arr),
        block=(3, 3, 1)
        )
```

Here the `SourceModule` is used to write a kernel in the CUDA C language and is then invokes from python.

## Performing Element-wise operations  

PyCuda also provides abstractions that can help perform element-wise operations on arrays without having to write too much boilerplate. It is done from the `pycuda.elementwise.ElementwiseKernel` object.

```python
from pycuda.elementwise import ElementwiseKernel

def elwise_ops(arr):
    arr_gpu = gpuarray.to_gpu(arr)

    ele_kernel = ElementwiseKernel(
        "float *x",
        "x[i] *= 2",
        "doubled")
    ele_kernel(arr_gpu)

    gpu_doubled = arr_gpu.get()
```

We can specify the input type of the array and the operation to be performed and PyCuda takes care of writing the kernel.

## Map Reduce on arrays  

A Numpy array can be reduced with the abstractions in PyCuda. This functionality is provided through `pycuda.reduction.ReductionKernel`

```python
from pycuda.reduction import ReductionKernel

def map_reduce(arr):
    arr_gpu = gpuarray.to_gpu(arr)

    doubled_sum = ReductionKernel(
        np.float32,
        neutral="0",
        reduce_expr="a+b",
        map_expr="x[i]*2",
        arguments="float *x")

    gpu_res = doubled_sum(arr_gpu).get()
```

We can specify operations to be performed for both map and reduce along with other options. The reduce uses `a` and `b` to denote the two values on which the reduction will be done for all the elements.