---
title: Movie Updates - Scrapy
layout: post
icon : fa-code 
---

The internet contains a wealth of data. Although there exists products and services that update about changes the do not cover all aspects. In this example we will keep up to date on all the latest movies playing in theaters from [IMDb](https://www.imdb.com/).

## Scraping web data 

Python provides an excellent framework called [Scrapy](https://scrapy.org/) which provides a complete workflow for scraping data from the web. It provides a default template for creating scraping projects but also supports creating a self contained spider.  
We initialize a project with the command `scrapy startproject [project name]` then a spider can be created with `scrapy genspider [name] [start URL]`.

## Getting the movie list  

Now that we have a project we get the list of movies playing in theaters from IMDb. In the spider file `project_name/spiders/spider_name.py` we add this code

```python
name = 'now_playing'
allowed_domains = ['imdb.com']
start_urls = ['https://imdb.com/movies-in-theaters/']

def parse(self, response):
    # Find both sections
    sections = response.xpath("//div[@class='list detail sub-list']")
    # For each section
    for section in sections:
        # Add section title as meta
        section_meta = {"section_name": section.xpath(".//h3/text()").extract_first()}
        # Find the movie links
        page_links = section.xpath(".//h4/a/@href").extract()
        # Process each link
        for link in page_links:
            yield Request(
                response.urljoin(link),
                callback=self.parse_movie_page,
                meta=section_meta
                )
```

The links of the movie pages are identified by their xpath. This scrapes the URLs form the page and passes them to the `parse_movie_page` method of the class to extract details of the movie. Also, we send the category name as the `response.meta` which is a dictionary to hold values.

## Getting the movie information  

Now that we have the link to the movie page we get the information about it using the `parse_movie_page` method.

```python
def parse_movie_page(self, response):
    # Initialize item loader
    movie_detail_loader = ItemLoader(item=MovieDetails(), response=response)
    # Find fields
    title = response.xpath("//h1[@itemprop='name']/text()").extract_first()
    rating = response.xpath("//span[@itemprop='ratingValue']/text()").extract_first()
    director = response.xpath("//span[@itemprop='director']/a/span/text()").extract_first()
    actors = response.xpath("//span[@itemprop='actors']/a/span/text()").extract()
    desc = response.xpath("//div[@class='summary_text']/text()").extract_first()
    # Add fields
    movie_detail_loader.add_value('category', response.meta['section_name'])
    movie_detail_loader.add_value('title', title)
    movie_detail_loader.add_value('rating', rating)
    if director:
        movie_detail_loader.add_value('director', director)
    else:
        movie_detail_loader.add_value('director', 'N/A')
    if actors:
        movie_detail_loader.add_value('actors', actors)
    else:
        movie_detail_loader.add_value('actors', 'N/A')
    if desc:
        movie_detail_loader.add_value('description', desc)
    else:
        movie_detail_loader.add_value('description', 'N/A')

    return movie_detail_loader.load_item()
```

Here, instead of using a simple dictionary we use a `scrapy.Item` which is also a dictionary but within scrapy framework. The items are loaded into the dictionary by the `scrapy.loader.ItemLoader`.  
The item is defined in the `items.py` file of the project as

```python
class MovieDetails(scrapy.Item):
    category = scrapy.Field()
    title = scrapy.Field()
    rating = scrapy.Field()
    director = scrapy.Field()
    actors = scrapy.Field()
    description = scrapy.Field()
```

It is, currently not more than a dictionary but can be extended by inbound and outbound processors.

## Cleaning up the data  

Now the data extracted can be further cleaned up to make it easier for other operations. This can be done by the item processors but we use the `pipelines` provided by scrapy. In the `pipelines.py` file of the project we define the pipeline as

```python
class MovieDetailsCleanup(object):
    def process_item(self, item, spider):
        item['category'] = item['category'][0].replace("\xa0", "")
        item['title'] = item['title'][0].replace("\xa0", "")
        item['rating'] = float(item['rating'][0])
        item['director'] = item['director'][0]
        item['actors'] = ", ".join(item['actors'])
        item['description'] = item['description'][0].strip()
        return item
```

This just cleans up the strings and converts the ratings to a `float`. Other operations can be done too like storing data to a database or a file.

Then the pipeline is activated in the `settings.py` file of the project

```python
ITEM_PIPELINES = {
    'movie_updates.pipelines.MovieDetailsCleanup': 300,
}
```

## Output the data to file  

Scrapy has native support to output data to files. This can be done when running the project by passing the `-o` argument

```bash
scrapy crawl [spider name] -o [output filename].[extension]
```

Scrapy supports many formats such as csv, json and xml.